{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJU3UQhLxDrnwXSYyxvvWO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeepgundeboina/LearningSpark/blob/main/SparkUnionAll.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdO_3T6jlKX_",
        "outputId": "8ba691f0-5b5d-4a3b-b32f-23014d151723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('SparkUnionAll').getOrCreate()"
      ],
      "metadata": {
        "id": "2aQCuhlcmQZE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=[(300,'Ram'),\n",
        "      (400,'Raju'),\n",
        "      (20,'Sevanand')]\n",
        "Schema=['Id','Name']\n",
        "df=spark.createDataFrame(data,Schema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqhQH5DNmZ0M",
        "outputId": "2347ea13-6c26-4d5b-d0e9-0c25bb3d6ee0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+\n",
            "| Id|    Name|\n",
            "+---+--------+\n",
            "|300|     Ram|\n",
            "|400|    Raju|\n",
            "| 20|Sevanand|\n",
            "+---+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1=[(300,'Ram'),\n",
        "      (40,'Ravi'),\n",
        "      (201,'Nandha')]\n",
        "Schema=['Id','Name']\n",
        "df1=spark.createDataFrame(data1,Schema)\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMZLa26hmvFq",
        "outputId": "eb634c0e-84d4-40f9-e197-e92b9a985201"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| Id|  Name|\n",
            "+---+------+\n",
            "|300|   Ram|\n",
            "| 40|  Ravi|\n",
            "|201|Nandha|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Syntax: Union"
      ],
      "metadata": {
        "id": "dNtrNtUNnCGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_union=df.union(df1)\n",
        "df_union.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvrYIcd0nAu5",
        "outputId": "60cc7a71-035d-4812-e264-18016b355933"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+\n",
            "| Id|    Name|\n",
            "+---+--------+\n",
            "|300|     Ram|\n",
            "|400|    Raju|\n",
            "| 20|Sevanand|\n",
            "|300|     Ram|\n",
            "| 40|    Ravi|\n",
            "|201|  Nandha|\n",
            "+---+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Synatx: Union All"
      ],
      "metadata": {
        "id": "xbYZ20vCnTxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_unionAll=df.unionAll(df1)\n",
        "df_unionAll.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5Ofm8AHnPVg",
        "outputId": "5eb64b0d-39b2-4f4c-9b4e-ec7a4484f251"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+\n",
            "| Id|    Name|\n",
            "+---+--------+\n",
            "|300|     Ram|\n",
            "|400|    Raju|\n",
            "| 20|Sevanand|\n",
            "|300|     Ram|\n",
            "| 40|    Ravi|\n",
            "|201|  Nandha|\n",
            "+---+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   As we are using spark version greater than 2.2 version there is no difference between Union and Union all\n",
        "*   In order to remove duplicates we have to use dropDuplicates()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vFUKONExnaPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_unionAll1=df.unionAll(df1).dropDuplicates()\n",
        "df_unionAll1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIro6D5AnYid",
        "outputId": "a0f4b32d-aa41-4cb2-97a0-4cd84930d2d7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+\n",
            "| Id|    Name|\n",
            "+---+--------+\n",
            "|300|     Ram|\n",
            "|400|    Raju|\n",
            "| 20|Sevanand|\n",
            "|201|  Nandha|\n",
            "| 40|    Ravi|\n",
            "+---+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   Schema of 2 dataframes should be same in order to perform Union\n",
        "\n"
      ],
      "metadata": {
        "id": "Uji96XcdoClh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data3=[(300,'Ram','40k'),\n",
        "      (400,'Raju','20K'),\n",
        "      (20,'Sevanand','5K')]\n",
        "Schema=['Id','Name','Salary']\n",
        "df4=spark.createDataFrame(data3,Schema)\n",
        "df4.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBvT3xu8n4Bo",
        "outputId": "80df2702-aff8-44a2-b08d-1c191855802d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+------+\n",
            "| Id|    Name|Salary|\n",
            "+---+--------+------+\n",
            "|300|     Ram|   40k|\n",
            "|400|    Raju|   20K|\n",
            "| 20|Sevanand|    5K|\n",
            "+---+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Mismatch error"
      ],
      "metadata": {
        "id": "pxrtjZt3os-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_mismatch=df1.union(df4)\n",
        "df_mismatch.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "HM9ZbmXxobYl",
        "outputId": "bdd895d0-28f6-43a4-cf15-cebe60cbc8fc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 2 columns and the second input has 3 columns.;\n'Union false, false\n:- LogicalRDD [Id#13L, Name#14], false\n+- LogicalRDD [Id#63L, Name#64, Salary#65], false\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-64fc28b5d97b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_mismatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_mismatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36munion\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   3925\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3926\u001b[0m         \"\"\"\n\u001b[0;32m-> 3927\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3929\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munionAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 2 columns and the second input has 3 columns.;\n'Union false, false\n:- LogicalRDD [Id#13L, Name#14], false\n+- LogicalRDD [Id#63L, Name#64, Salary#65], false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**END OF CODE**"
      ],
      "metadata": {
        "id": "j11l-eREolp2"
      }
    }
  ]
}