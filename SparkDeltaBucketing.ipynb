{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSw/71RilAIidr5whULkG6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeepgundeboina/LearningSpark/blob/main/SparkDeltaBucketing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaIDft5xELzH"
      },
      "outputs": [],
      "source": [
        "!pip install delta-spark==2.0.0\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SparkDeltaBucketing\") \\\n",
        "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.0.0\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .getOrCreate()\n",
        "from delta.tables import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "F2vX34vvE_fI",
        "outputId": "94cb6556-6015-4063-fb19-0e7ecf418365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7dd3bc102a90>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ab40798fcd96:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>SparkDeltaWindow</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.get('spark.sql.sources.bucketing.enabled')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MB0_RvkUFw7K",
        "outputId": "5767b093-25d5-4a0f-e3e7-cf97d5623ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'true'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, rand\n",
        "df=spark.range(1,10000,1,10).select(col('id').alias('PK'),rand(10).alias('Attribute'))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TX-DFav3FS90",
        "outputId": "3f414b3d-f2b6-4272-ed59-2e6a01d290ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------+\n",
            "| PK|          Attribute|\n",
            "+---+-------------------+\n",
            "|  1| 0.1709497137955568|\n",
            "|  2| 0.8051143958005459|\n",
            "|  3| 0.5775925576589018|\n",
            "|  4| 0.9476047869880925|\n",
            "|  5|    0.2093704977577|\n",
            "|  6|0.36664222617947817|\n",
            "|  7| 0.8078688178371882|\n",
            "|  8| 0.7135143433452461|\n",
            "|  9| 0.7195325566306053|\n",
            "| 10|0.31335292311175456|\n",
            "| 11| 0.8062503712025726|\n",
            "| 12|0.10814914646176654|\n",
            "| 13| 0.3362232980701172|\n",
            "| 14| 0.8133304803837667|\n",
            "| 15|0.47649428738170896|\n",
            "| 16|  0.524728096293865|\n",
            "| 17| 0.9701253460019921|\n",
            "| 18| 0.6232167713919952|\n",
            "| 19| 0.5089687568245219|\n",
            "| 20| 0.5467504094508642|\n",
            "+---+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7kSjgYCFs7D",
        "outputId": "a0f26b2e-99c6-400c-86aa-e0bfd39c2c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9999"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.format('parquet').saveAsTable('non_bucketed_table')"
      ],
      "metadata": {
        "id": "Iy1AutJfF98o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JDtvvMHGVjY",
        "outputId": "638b8e98-fa02-4e80-a9f1-530f52ae7d5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.format('parquet').bucketBy(10,'PK').saveAsTable('bucketed_table')"
      ],
      "metadata": {
        "id": "zYDHk3YoGfeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1=spark.table('bucketed_table')\n",
        "df2=spark.table('bucketed_table')\n",
        "\n",
        "df3=spark.table('non_bucketed_table')\n",
        "df4=spark.table('non_bucketed_table')"
      ],
      "metadata": {
        "id": "n9CEBvZxGr6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.join(df4,'PK','inner').explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0Y18ygaHYI_",
        "outputId": "79c8e452-797d-4aec-a10a-ed30dee22645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [PK#206L, Attribute#207, Attribute#213]\n",
            "   +- BroadcastHashJoin [PK#206L], [PK#212L], Inner, BuildRight, false\n",
            "      :- Filter isnotnull(PK#206L)\n",
            "      :  +- FileScan parquet default.non_bucketed_table[PK#206L,Attribute#207] Batched: true, DataFilters: [isnotnull(PK#206L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/spark-warehouse/non_bucketed_table], PartitionFilters: [], PushedFilters: [IsNotNull(PK)], ReadSchema: struct<PK:bigint,Attribute:double>\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=125]\n",
            "         +- Filter isnotnull(PK#212L)\n",
            "            +- FileScan parquet default.non_bucketed_table[PK#212L,Attribute#213] Batched: true, DataFilters: [isnotnull(PK#212L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/spark-warehouse/non_bucketed_table], PartitionFilters: [], PushedFilters: [IsNotNull(PK)], ReadSchema: struct<PK:bigint,Attribute:double>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set('spark.sql.autoBroadcastJoinThreshold',-1)\n",
        "spark.conf.set('spark.sql.adaptive.enabled',False)"
      ],
      "metadata": {
        "id": "Jqs_0AvnHrGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.join(df4,'PK','inner').explain(mode='formatted')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb1j7kvGItzQ",
        "outputId": "d3f53c80-848c-4d09-e6e0-7345e6d039d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "* Project (9)\n",
            "+- * SortMergeJoin Inner (8)\n",
            "   :- * Sort (5)\n",
            "   :  +- Exchange (4)\n",
            "   :     +- * Filter (3)\n",
            "   :        +- * ColumnarToRow (2)\n",
            "   :           +- Scan parquet default.non_bucketed_table (1)\n",
            "   +- * Sort (7)\n",
            "      +- ReusedExchange (6)\n",
            "\n",
            "\n",
            "(1) Scan parquet default.non_bucketed_table\n",
            "Output [2]: [PK#206L, Attribute#207]\n",
            "Batched: true\n",
            "Location: InMemoryFileIndex [file:/content/spark-warehouse/non_bucketed_table]\n",
            "PushedFilters: [IsNotNull(PK)]\n",
            "ReadSchema: struct<PK:bigint,Attribute:double>\n",
            "\n",
            "(2) ColumnarToRow [codegen id : 1]\n",
            "Input [2]: [PK#206L, Attribute#207]\n",
            "\n",
            "(3) Filter [codegen id : 1]\n",
            "Input [2]: [PK#206L, Attribute#207]\n",
            "Condition : isnotnull(PK#206L)\n",
            "\n",
            "(4) Exchange\n",
            "Input [2]: [PK#206L, Attribute#207]\n",
            "Arguments: hashpartitioning(PK#206L, 200), ENSURE_REQUIREMENTS, [plan_id=261]\n",
            "\n",
            "(5) Sort [codegen id : 2]\n",
            "Input [2]: [PK#206L, Attribute#207]\n",
            "Arguments: [PK#206L ASC NULLS FIRST], false, 0\n",
            "\n",
            "(6) ReusedExchange [Reuses operator id: 4]\n",
            "Output [2]: [PK#222L, Attribute#223]\n",
            "\n",
            "(7) Sort [codegen id : 4]\n",
            "Input [2]: [PK#222L, Attribute#223]\n",
            "Arguments: [PK#222L ASC NULLS FIRST], false, 0\n",
            "\n",
            "(8) SortMergeJoin [codegen id : 5]\n",
            "Left keys [1]: [PK#206L]\n",
            "Right keys [1]: [PK#222L]\n",
            "Join condition: None\n",
            "\n",
            "(9) Project [codegen id : 5]\n",
            "Output [3]: [PK#206L, Attribute#207, Attribute#223]\n",
            "Input [4]: [PK#206L, Attribute#207, PK#222L, Attribute#223]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.join(df3,'PK','inner').explain(mode='formatted')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6iDtlfZIw5d",
        "outputId": "43c09590-920d-423a-99ed-cc1f519f529b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "* Project (12)\n",
            "+- * SortMergeJoin Inner (11)\n",
            "   :- * Sort (5)\n",
            "   :  +- Exchange (4)\n",
            "   :     +- * Filter (3)\n",
            "   :        +- * ColumnarToRow (2)\n",
            "   :           +- Scan parquet default.bucketed_table (1)\n",
            "   +- * Sort (10)\n",
            "      +- Exchange (9)\n",
            "         +- * Filter (8)\n",
            "            +- * ColumnarToRow (7)\n",
            "               +- Scan parquet default.non_bucketed_table (6)\n",
            "\n",
            "\n",
            "(1) Scan parquet default.bucketed_table\n",
            "Output [2]: [PK#194L, Attribute#195]\n",
            "Batched: true\n",
            "Location: InMemoryFileIndex [file:/content/spark-warehouse/bucketed_table]\n",
            "PushedFilters: [IsNotNull(PK)]\n",
            "ReadSchema: struct<PK:bigint,Attribute:double>\n",
            "\n",
            "(2) ColumnarToRow [codegen id : 1]\n",
            "Input [2]: [PK#194L, Attribute#195]\n",
            "\n",
            "(3) Filter [codegen id : 1]\n",
            "Input [2]: [PK#194L, Attribute#195]\n",
            "Condition : isnotnull(PK#194L)\n",
            "\n",
            "(4) Exchange\n",
            "Input [2]: [PK#194L, Attribute#195]\n",
            "Arguments: hashpartitioning(PK#194L, 200), ENSURE_REQUIREMENTS, [plan_id=354]\n",
            "\n",
            "(5) Sort [codegen id : 2]\n",
            "Input [2]: [PK#194L, Attribute#195]\n",
            "Arguments: [PK#194L ASC NULLS FIRST], false, 0\n",
            "\n",
            "(6) Scan parquet default.non_bucketed_table\n",
            "Output [2]: [PK#206L, Attribute#207]\n",
            "Batched: true\n",
            "Location: InMemoryFileIndex [file:/content/spark-warehouse/non_bucketed_table]\n",
            "PushedFilters: [IsNotNull(PK)]\n",
            "ReadSchema: struct<PK:bigint,Attribute:double>\n",
            "\n",
            "(7) ColumnarToRow [codegen id : 3]\n",
            "Input [2]: [PK#206L, Attribute#207]\n",
            "\n",
            "(8) Filter [codegen id : 3]\n",
            "Input [2]: [PK#206L, Attribute#207]\n",
            "Condition : isnotnull(PK#206L)\n",
            "\n",
            "(9) Exchange\n",
            "Input [2]: [PK#206L, Attribute#207]\n",
            "Arguments: hashpartitioning(PK#206L, 200), ENSURE_REQUIREMENTS, [plan_id=363]\n",
            "\n",
            "(10) Sort [codegen id : 4]\n",
            "Input [2]: [PK#206L, Attribute#207]\n",
            "Arguments: [PK#206L ASC NULLS FIRST], false, 0\n",
            "\n",
            "(11) SortMergeJoin [codegen id : 5]\n",
            "Left keys [1]: [PK#194L]\n",
            "Right keys [1]: [PK#206L]\n",
            "Join condition: None\n",
            "\n",
            "(12) Project [codegen id : 5]\n",
            "Output [3]: [PK#194L, Attribute#195, Attribute#207]\n",
            "Input [4]: [PK#194L, Attribute#195, PK#206L, Attribute#207]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.join(df1,'PK','inner').explain(mode='formatted')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FVxczOuKCuR",
        "outputId": "86c92e6a-df69-43ed-ea1e-2170fedf8705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "* Project (10)\n",
            "+- * SortMergeJoin Inner (9)\n",
            "   :- * Sort (4)\n",
            "   :  +- * Filter (3)\n",
            "   :     +- * ColumnarToRow (2)\n",
            "   :        +- Scan parquet default.bucketed_table (1)\n",
            "   +- * Sort (8)\n",
            "      +- * Filter (7)\n",
            "         +- * ColumnarToRow (6)\n",
            "            +- Scan parquet default.bucketed_table (5)\n",
            "\n",
            "\n",
            "(1) Scan parquet default.bucketed_table\n",
            "Output [2]: [PK#194L, Attribute#195]\n",
            "Batched: true\n",
            "Location: InMemoryFileIndex [file:/content/spark-warehouse/bucketed_table]\n",
            "PushedFilters: [IsNotNull(PK)]\n",
            "ReadSchema: struct<PK:bigint,Attribute:double>\n",
            "SelectedBucketsCount: 10 out of 10\n",
            "\n",
            "(2) ColumnarToRow [codegen id : 1]\n",
            "Input [2]: [PK#194L, Attribute#195]\n",
            "\n",
            "(3) Filter [codegen id : 1]\n",
            "Input [2]: [PK#194L, Attribute#195]\n",
            "Condition : isnotnull(PK#194L)\n",
            "\n",
            "(4) Sort [codegen id : 1]\n",
            "Input [2]: [PK#194L, Attribute#195]\n",
            "Arguments: [PK#194L ASC NULLS FIRST], false, 0\n",
            "\n",
            "(5) Scan parquet default.bucketed_table\n",
            "Output [2]: [PK#230L, Attribute#231]\n",
            "Batched: true\n",
            "Location: InMemoryFileIndex [file:/content/spark-warehouse/bucketed_table]\n",
            "PushedFilters: [IsNotNull(PK)]\n",
            "ReadSchema: struct<PK:bigint,Attribute:double>\n",
            "SelectedBucketsCount: 10 out of 10\n",
            "\n",
            "(6) ColumnarToRow [codegen id : 2]\n",
            "Input [2]: [PK#230L, Attribute#231]\n",
            "\n",
            "(7) Filter [codegen id : 2]\n",
            "Input [2]: [PK#230L, Attribute#231]\n",
            "Condition : isnotnull(PK#230L)\n",
            "\n",
            "(8) Sort [codegen id : 2]\n",
            "Input [2]: [PK#230L, Attribute#231]\n",
            "Arguments: [PK#230L ASC NULLS FIRST], false, 0\n",
            "\n",
            "(9) SortMergeJoin [codegen id : 3]\n",
            "Left keys [1]: [PK#194L]\n",
            "Right keys [1]: [PK#230L]\n",
            "Join condition: None\n",
            "\n",
            "(10) Project [codegen id : 3]\n",
            "Output [3]: [PK#194L, Attribute#195, Attribute#231]\n",
            "Input [4]: [PK#194L, Attribute#195, PK#230L, Attribute#231]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**END OF CODE**"
      ],
      "metadata": {
        "id": "AQzgDNXKK4eL"
      }
    }
  ]
}