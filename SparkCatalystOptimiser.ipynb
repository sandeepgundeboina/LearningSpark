{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3GATgLWW1Qbc0bCs96eoA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeepgundeboina/LearningSpark/blob/main/SparkCatalystOptimiser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K1QbyjTDYKeM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb235976-fe50-4dbf-87cb-9832f9834a10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName('SparkCatalystOptimiser').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "f063mueht6qE",
        "outputId": "50e8710a-b916-40ce-ad43-b95728943222"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7b84e05076d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ec811e9353d8:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>SparkCatalystOptimiser</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "data=[('Ram',2339943,'24-03-2022','04-03-1998',40000,'A','Marketing'),\n",
        "      ('Raju',244335,'04-06-2021','22-08-1994',20000,'A','Sales'),\n",
        "      ('Charan',254335,'29-03-2024','11-07-1996',25000,'A','Marketing'),\n",
        "      ('Devdas',12343,'21-01-2021','04-05-1993',55000,'R','Sales'),\n",
        "      ('Rezza',124335,'15-08-2024','03-12-1994',65000,'A','Sales')\n",
        "      ]\n",
        "schema=['Name','Emp_id','Joining_date','DOB','Salary','Status','Dept']\n",
        "schem1=StructType([\n",
        "    StructField('Name',StringType()),\n",
        "    StructField('Emp_id',IntegerType()),\n",
        "    StructField('Joining_date',DateType()),\n",
        "    StructField('DOB',DateType()),\n",
        "    StructField('Salary',IntegerType()),\n",
        "    StructField('Status',StringType()),\n",
        "    StructField('Dept',StringType())\n",
        "])\n",
        "df=spark.createDataFrame(data=data,schema=schema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-efIRxPVuA1z",
        "outputId": "4242cdfb-e7bf-4ccd-c591-a452a1c48844"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+------------+----------+------+------+---------+\n",
            "|  Name| Emp_id|Joining_date|       DOB|Salary|Status|     Dept|\n",
            "+------+-------+------------+----------+------+------+---------+\n",
            "|   Ram|2339943|  24-03-2022|04-03-1998| 40000|     A|Marketing|\n",
            "|  Raju| 244335|  04-06-2021|22-08-1994| 20000|     A|    Sales|\n",
            "|Charan| 254335|  29-03-2024|11-07-1996| 25000|     A|Marketing|\n",
            "|Devdas|  12343|  21-01-2021|04-05-1993| 55000|     R|    Sales|\n",
            "| Rezza| 124335|  15-08-2024|03-12-1994| 65000|     A|    Sales|\n",
            "+------+-------+------------+----------+------+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = df.filter(df.Status=='A').groupBy(df.Dept).count()\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1h9eafP4iv5",
        "outputId": "6a642e8a-34ec-4f39-cb67-89d4cbaff090"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|     Dept|count|\n",
            "+---------+-----+\n",
            "|    Sales|    2|\n",
            "|Marketing|    2|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "explain function is used to get the plan that is used for the query"
      ],
      "metadata": {
        "id": "4wwp7zSuBCF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result.explain(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeVr47xiAwF4",
        "outputId": "c5b7e099-de6a-4b60-e648-ff80efc0ab9a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[Dept#86], functions=[count(1)])\n",
            "   +- Exchange hashpartitioning(Dept#86, 200), ENSURE_REQUIREMENTS, [plan_id=103]\n",
            "      +- HashAggregate(keys=[Dept#86], functions=[partial_count(1)])\n",
            "         +- Project [Dept#86]\n",
            "            +- Filter (isnotnull(Status#85) AND (Status#85 = A))\n",
            "               +- Scan ExistingRDD[Name#80,Emp_id#81L,Joining_date#82,DOB#83,Salary#84L,Status#85,Dept#86]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    To get deatiled plan"
      ],
      "metadata": {
        "id": "ZcuISde_DnUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhNgxZw_A0Ub",
        "outputId": "9c509595-d8e6-46d2-a8b9-22a053984aac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "Aggregate [Dept#86], [Dept#86, count(1) AS count#131L]\n",
            "+- Filter (Status#85 = A)\n",
            "   +- LogicalRDD [Name#80, Emp_id#81L, Joining_date#82, DOB#83, Salary#84L, Status#85, Dept#86], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "Dept: string, count: bigint\n",
            "Aggregate [Dept#86], [Dept#86, count(1) AS count#131L]\n",
            "+- Filter (Status#85 = A)\n",
            "   +- LogicalRDD [Name#80, Emp_id#81L, Joining_date#82, DOB#83, Salary#84L, Status#85, Dept#86], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [Dept#86], [Dept#86, count(1) AS count#131L]\n",
            "+- Project [Dept#86]\n",
            "   +- Filter (isnotnull(Status#85) AND (Status#85 = A))\n",
            "      +- LogicalRDD [Name#80, Emp_id#81L, Joining_date#82, DOB#83, Salary#84L, Status#85, Dept#86], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[Dept#86], functions=[count(1)], output=[Dept#86, count#131L])\n",
            "   +- Exchange hashpartitioning(Dept#86, 200), ENSURE_REQUIREMENTS, [plan_id=103]\n",
            "      +- HashAggregate(keys=[Dept#86], functions=[partial_count(1)], output=[Dept#86, count#141L])\n",
            "         +- Project [Dept#86]\n",
            "            +- Filter (isnotnull(Status#85) AND (Status#85 = A))\n",
            "               +- Scan ExistingRDD[Name#80,Emp_id#81L,Joining_date#82,DOB#83,Salary#84L,Status#85,Dept#86]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    it will provide us the steps, how spark engine is approaching for agiven query.\n",
        "\n",
        "    Always read the plan from bottom to top.\n",
        "\n",
        "    Plan is followed from bottom to top"
      ],
      "metadata": {
        "id": "Y2JQZCtFEIHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**END OF CODE**"
      ],
      "metadata": {
        "id": "kLbT9njqEix2"
      }
    }
  ]
}